#!/usr/bin/env python

from prompt_toolkit import prompt
from prompt_toolkit.formatted_text import HTML
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig


def prompt_continuation(width, line_number, wrap_count):
    """
    It displays line numbers and an arrow for wrapped lines.
    """
    if wrap_count > 0:
        return " " * (width - 3) + "-> "
    else:
        text = ("- %i - " % (line_number)).ljust(width)
        return HTML("<strong>%s</strong>") % text


def barra_cuda():
    if not torch.cuda.is_available():
        raise RuntimeError("No CUDA device found. Run: lspci | grep -i vga")
    else:
        print("CUDA is available! Using GPU:", torch.cuda.get_device_name(0))


def main():
    print(
        "Prompt Editor (Press [Meta+Enter] or [Esc] then [Enter] to finish):\n")
    user_prompt = prompt(">>> ", multiline=True,
                         prompt_continuation=prompt_continuation)
    if not user_prompt.strip():
        print("No prompt entered. Exiting.")
        return

    print("\nYour prompt was:\n------------------------------------------")
    print(user_prompt)
    print("------------------------------------------\n")
    barra_cuda()
    model_name = "Qwen/Qwen2.5-Coder-3B"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    quant_cfg = BitsAndBytesConfig(load_in_8bit=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=quant_cfg,
        device_map="auto"
    )
    inputs = tokenizer(user_prompt, return_tensors="pt",
                       padding=True, truncation=True)
    inputs = {key: tensor.to("cuda") for key, tensor in inputs.items()}
    outputs = model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=256,
        do_sample=True,
        temperature=0.1,
    )
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print("Generated Output:\n------------------------------------------")
    print(generated_text)
    print("------------------------------------------")


if __name__ == "__main__":
    main()
